{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9f2179",
   "metadata": {
    "_cell_guid": "33b5dd02-a5f0-496c-8d77-8e6cab5571dd",
    "_uuid": "a6950905-7e44-4f99-80c4-6497b3cd7af7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-24T17:39:39.490466Z",
     "iopub.status.busy": "2025-08-24T17:39:39.490061Z",
     "iopub.status.idle": "2025-08-24T17:39:41.438611Z",
     "shell.execute_reply": "2025-08-24T17:39:41.437430Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.953667,
     "end_time": "2025-08-24T17:39:41.440278",
     "exception": false,
     "start_time": "2025-08-24T17:39:39.486611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic-machine-learning-u-lima/train.csv\n",
      "/kaggle/input/titanic-machine-learning-u-lima/test.csv\n",
      "/kaggle/input/titanic-machine-learning-u-lima/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3a8c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T17:39:41.445725Z",
     "iopub.status.busy": "2025-08-24T17:39:41.445246Z",
     "iopub.status.idle": "2025-08-24T17:40:48.570792Z",
     "shell.execute_reply": "2025-08-24T17:40:48.569865Z"
    },
    "papermill": {
     "duration": 67.130927,
     "end_time": "2025-08-24T17:40:48.573234",
     "exception": false,
     "start_time": "2025-08-24T17:39:41.442307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV to find the best model...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best parameters found: {'classifier__max_depth': 15, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 300}\n",
      "Best cross-validation accuracy: 83.50%\n",
      "\n",
      "Successfully created and saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "# Define paths for the dataset\n",
    "train_path = \"/kaggle/input/titanic-machine-learning-u-lima/train.csv\"\n",
    "test_path  = \"/kaggle/input/titanic-machine-learning-u-lima/test.csv\"\n",
    "\n",
    "# Read the training and testing data\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# Store PassengerIds from the test set for the final submission file\n",
    "test_passenger_ids = test_df[\"PassengerId\"]\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "# We'll create new features from the existing ones to improve model accuracy.\n",
    "# This function will be applied to both train and test sets for consistency.\n",
    "\n",
    "def engineer_features(df):\n",
    "    # Extract titles from the 'Name' column (e.g., Mr, Mrs, Miss)\n",
    "    df['Title'] = df['Name'].apply(lambda x: re.search(' ([A-Za-z]+)\\.', x).group(1))\n",
    "    \n",
    "    # Consolidate rare titles into more common categories\n",
    "    title_mapping = {\n",
    "        \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\", \"Dr\": \"Officer\", \n",
    "        \"Rev\": \"Officer\", \"Col\": \"Officer\", \"Major\": \"Officer\", \n",
    "        \"Capt\": \"Officer\", \"Lady\": \"Royalty\", \"Countess\": \"Royalty\", \n",
    "        \"Jonkheer\": \"Royalty\", \"Sir\": \"Royalty\", \"Don\": \"Royalty\", \"Dona\": \"Royalty\"\n",
    "    }\n",
    "    df['Title'] = df['Title'].replace(title_mapping)\n",
    "\n",
    "    # Create 'FamilySize' by combining SibSp and Parch\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    \n",
    "    # Create 'IsAlone' feature\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "    # Extract the first letter of the 'Cabin' (Deck), fill missing with 'U' for Unknown\n",
    "    df['Deck'] = df['Cabin'].str[0].fillna('U')\n",
    "\n",
    "    # Drop columns that are no longer needed after feature engineering\n",
    "    df = df.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the feature engineering function\n",
    "train_df = engineer_features(train_df)\n",
    "test_df = engineer_features(test_df)\n",
    "\n",
    "# --- 3. Preprocessing ---\n",
    "# Separate target variable (y) from features (X)\n",
    "y = train_df[\"Survived\"]\n",
    "X = train_df.drop(columns=[\"Survived\"])\n",
    "\n",
    "# Align columns between training and testing sets, in case of any discrepancies\n",
    "# This ensures the test set has the same columns as the training set after one-hot encoding\n",
    "train_cols = X.columns\n",
    "test_cols = test_df.columns\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    test_df[c] = 0\n",
    "test_df = test_df[train_cols]\n",
    "\n",
    "\n",
    "# Define categorical and numerical features\n",
    "# Note: 'Pclass' is treated as a categorical feature\n",
    "cat_features = [\"Sex\", \"Embarked\", \"Title\", \"Deck\", \"Pclass\"]\n",
    "num_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"IsAlone\"]\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "# Numerical features will be imputed with the median and then scaled\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features will be imputed with the most frequent value and then one-hot encoded\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_features),\n",
    "        (\"cat\", categorical_transformer, cat_features),\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (if any)\n",
    ")\n",
    "\n",
    "# --- 4. Model Training and Hyperparameter Tuning ---\n",
    "# We will use a RandomForestClassifier, a powerful ensemble model.\n",
    "# We'll use GridSearchCV to find the best combination of hyperparameters.\n",
    "\n",
    "# Create the full pipeline including preprocessing and the model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "# This is a focused grid to balance performance and computation time\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV to perform a 5-fold cross-validation search\n",
    "grid_search = GridSearchCV(\n",
    "    model_pipeline, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(\"Starting GridSearchCV to find the best model...\")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the best cross-validation score\n",
    "print(\"\\nBest parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "# --- 5. Generate Predictions and Submission File ---\n",
    "# The best model found by GridSearchCV is automatically refit on the entire dataset.\n",
    "# We use this best estimator to make predictions on the test data.\n",
    "best_model = grid_search.best_estimator_\n",
    "test_predictions = best_model.predict(test_df)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_passenger_ids,\n",
    "    \"Survived\": test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nSuccessfully created and saved submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13463071,
     "sourceId": 113103,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 76.923973,
   "end_time": "2025-08-24T17:40:51.196007",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-24T17:39:34.272034",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
